{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nuclear Segmentation Pipelines","text":"<p>The GoNuclear repository hosts the code and guides for the pipelines used in the paper A deep learning-based toolkit for 3D nuclei segmentation and quantitative analysis in cellular and tissue context. It is structured in to four folders:</p> <ul> <li>stardist/ contains a 3D StarDist training and inference pipeline, <code>run-stardist</code>.</li> <li>plantseg/ contains configuration files for training and inference with PlantSeg.</li> <li>cellpose/ contains scripts for training and inference with Cellpose.</li> <li>evaluation/ contains modules for evaluating the segmentation results.</li> </ul> <p>and are described in GoNuclear documentation .</p>"},{"location":"#data-and-models","title":"Data and Models","text":"<p>Please go to BioImage Archive S-BIAD1026 for the training data and models. I organised them in the following structure:</p> <pre><code>Training data\n\u251c\u2500\u2500 2d/\n\u2502   \u251c\u2500\u2500 isotropic/\n\u2502   \u2502   \u251c\u2500\u2500 gold/\n\u2502   \u2502   \u2514\u2500\u2500 initial/\n\u2502   \u2514\u2500\u2500 original/\n\u2502       \u251c\u2500\u2500 gold/\n\u2502       \u2514\u2500\u2500 README.txt\n\u2514\u2500\u2500 3d_all_in_one/\n    \u251c\u2500\u2500 1135.h5\n    \u251c\u2500\u2500 1136.h5\n    \u251c\u2500\u2500 1137.h5\n    \u251c\u2500\u2500 1139.h5\n    \u2514\u2500\u2500 1170.h5\n\nModels\n\u251c\u2500\u2500 cellpose/\n\u2502   \u251c\u2500\u2500 cyto2_finetune/\n\u2502   \u2502   \u2514\u2500\u2500 gold/\n\u2502   \u251c\u2500\u2500 nuclei_finetune/\n\u2502   \u2502   \u251c\u2500\u2500 gold/\n\u2502   \u2502   \u2514\u2500\u2500 initial/\n\u2502   \u2514\u2500\u2500 scratch_trained/\n\u2502       \u2514\u2500\u2500 gold/\n\u251c\u2500\u2500 plantseg/\n\u2502   \u2514\u2500\u2500 3dunet/\n\u2502       \u251c\u2500\u2500 gold/\n\u2502       \u251c\u2500\u2500 initial/\n\u2502       \u251c\u2500\u2500 platinum/\n\u2502       \u2514\u2500\u2500 train_example.yml\n\u2514\u2500\u2500 stardist/\n    \u251c\u2500\u2500 resnet/\n    \u2502   \u251c\u2500\u2500 gold/\n    \u2502   \u251c\u2500\u2500 initial/\n    \u2502   \u2514\u2500\u2500 platinum/\n    \u251c\u2500\u2500 train_example.yml\n    \u2514\u2500\u2500 unet/\n        \u2514\u2500\u2500 gold/\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>If you find this work useful, please cite our paper and the respective tools' papers:</p> <pre><code>@article{vijayan2024deep,\n  title={A deep learning-based toolkit for 3D nuclei segmentation and quantitative analysis in cellular and tissue context},\n  author={Vijayan, Athul and Mody, Tejasvinee Atul and Yu, Qin and Wolny, Adrian and Cerrone, Lorenzo and Strauss, Soeren and Tsiantis, Miltos and Smith, Richard S and Hamprecht, Fred A and Kreshuk, Anna and others},\n  journal={Development},\n  volume={151},\n  number={14},\n  year={2024},\n  publisher={The Company of Biologists}\n}\n</code></pre>"},{"location":"chapters/cellpose/","title":"Use Cellpose: A Guide","text":"<p>This part of the repo concisely shows how to install, train and segment with Cellpose. In other word, it is a record of how Cellpose is used in this paper. Since my experiments show StarDist and PlantSeg have better 3D segmentation performance than Cellpose, this section is complete yet not extensive.</p> <ul> <li>Installation<ul> <li>Install Miniconda</li> <li>Install <code>cellpose</code> using <code>pip</code></li> </ul> </li> <li>Segmentation<ul> <li>Data Preparation</li> <li>Segmentation Command</li> </ul> </li> <li>Training<ul> <li>Data Preparation</li> <li>Training Command</li> </ul> </li> <li>Cellpose Version and Code</li> <li>Cite</li> </ul>"},{"location":"chapters/cellpose/#installation","title":"Installation","text":"<p>It is recommended to install this package in an environment managed by <code>conda</code>. We start the guide by installing Mini-<code>conda</code>.</p>"},{"location":"chapters/cellpose/#install-miniconda","title":"Install Miniconda","text":"<p>First step required to use the pipeline is installing Miniconda. If you already have a working Anaconda setup you can go directly to the next step. Anaconda can be downloaded for all platforms from here. We suggest to use Miniconda, because it is lighter and install fewer unnecessary packages.</p> <p>To download Miniconda, open a terminal and type:</p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Then install by typing:</p> <pre><code>bash ./Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>and follow the installation instructions. The Miniconda3-latest-Linux-x86_64.sh file can be safely deleted.</p>"},{"location":"chapters/cellpose/#install-cellpose-using-pip","title":"Install <code>cellpose</code> using <code>pip</code>","text":"<p>To create and activate an <code>conda</code> environment for <code>cellpose</code>, then install <code>cellpose</code> in it, run the following commands in the terminal:</p> <pre><code>conda create --name cellpose python=3.8\nconda activate cellpose\npip install cellpose\n</code></pre> <p>If you have a nvidia gpu, follow these steps to make use of it:</p> <pre><code>pip uninstall torch\nconda install pytorch==1.12.0 cudatoolkit=11.3 -c pytorch\n</code></pre> <p>If you encounter error or need more explanation, go to Cellpose's official instruction.</p>"},{"location":"chapters/cellpose/#segmentation","title":"Segmentation","text":"<p>Although the PlantSeg and StarDist models from this study outperform the Cellpose models I trained. One may find the gold models in BioImage Archive S-BIAD1026, or one of them <code>philosophical-panda</code> at BioImage Model Zoo.</p>"},{"location":"chapters/cellpose/#data-preparation","title":"Data Preparation","text":"<p>Cellpose inference only segmenet TIFF images, not HDF5. However, it can take 3D volumes as input.</p>"},{"location":"chapters/cellpose/#segmentation-command","title":"Segmentation Command","text":"<p>There are two ways of segmenting 3D images with Cellpose:</p> <ul> <li> <p>Segment 3D images slice by slice then stitch 2D segmentation results into 3D segmentation results. With this approach, the images doesn't have to be isotropic, as long as the XY planes have similar properties as the training data.</p> <pre><code>cellpose \\\n    --pretrained_model PATH_TO_MODEL \\\n    --savedir PATH_TO_OUTPUT_DIR \\\n    --dir PATH_TO_3D_TIFF_FOLDER \\\n    --diameter 26.5 \\\n    --verbose \\\n    --use_gpu \\\n    --stitch_threshold 0.9 \\\n    --chan 0 \\\n    --no_npy \\\n    --save_tif\n</code></pre> </li> <li> <p>Compute spatial flow of 3D images in all dimensions then segment the images in 3D directly. You may choose to rescale the images to be isotropic before segmentation, or specify the anisotropy to let Cellpose deal with the rescaling. Here I show the later.</p> <pre><code>cellpose \\\n    --pretrained_model PATH_TO_MODEL \\\n    --savedir PATH_TO_OUTPUT_DIR \\\n    --dir PATH_TO_3D_TIFF_FOLDER \\\n    --diameter 26.5 \\\n    --anisotropy 2.24 \\\n    --verbose \\\n    --use_gpu \\\n    --do_3D \\\n    --chan 0 \\\n    --no_npy \\\n    --save_tif\n</code></pre> </li> </ul>"},{"location":"chapters/cellpose/#training","title":"Training","text":""},{"location":"chapters/cellpose/#data-preparation_1","title":"Data Preparation","text":"<p>Cellpose training only takes 2D images as input. To train on 3D images, we first need to split the 3D images into 2D images. Note that 3D images are better to be rescaled for isotropy in the resulting 2D training data.</p>"},{"location":"chapters/cellpose/#training-command","title":"Training Command","text":"<p>An example training command is shown below, which is used in the paper. The parameters <code>--learning_rate 0.1</code> and <code>--weight_decay 0.0001</code> are recommended by the Cellpose official documentation.</p> <pre><code>cellpose --train --use_gpu \\\n    --dir PATH_TO_TRAINING_DATA \\\n    --pretrained_model nuclei \\\n    --learning_rate 0.1 \\\n    --weight_decay 0.0001 \\\n    --mask_filter _masks \\\n    --verbose\n</code></pre>"},{"location":"chapters/cellpose/#cellpose-version-and-code","title":"Cellpose Version and Code","text":"<p>See Cellpose's GitHub page for the code. Cellpose v2.0.5 was used for training and inference in this paper.</p>"},{"location":"chapters/cellpose/#cite","title":"Cite","text":"<p>If you find the code/models/datasets useful, please cite our paper and Cellpose:</p> <pre><code>@article{vijayan2024deep,\n  title={A deep learning-based toolkit for 3D nuclei segmentation and quantitative analysis in cellular and tissue context},\n  author={Vijayan, Athul and Mody, Tejasvinee Atul and Yu, Qin and Wolny, Adrian and Cerrone, Lorenzo and Strauss, Soeren and Tsiantis, Miltos and Smith, Richard S and Hamprecht, Fred A and Kreshuk, Anna and others},\n  journal={Development},\n  volume={151},\n  number={14},\n  year={2024},\n  publisher={The Company of Biologists}\n}\n\n@article{stringer2021cellpose,\n  title={Cellpose: a generalist algorithm for cellular segmentation},\n  author={Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},\n  journal={Nature methods},\n  volume={18},\n  number={1},\n  pages={100--106},\n  year={2021},\n  publisher={Nature Publishing Group US New York}\n}\n</code></pre>"},{"location":"chapters/evaluation/","title":"Evaluation Module","text":"<p>This module contains the code for evaluating the performance of the trained models. This is an implementation of the scoring metirc in 2018 Data Science Bowl.</p>"},{"location":"chapters/evaluation/#cite","title":"Cite","text":"<p>If you find this work useful, please cite our paper:</p> <pre><code>@article{vijayan2024deep,\n  title={A deep learning-based toolkit for 3D nuclei segmentation and quantitative analysis in cellular and tissue context},\n  author={Vijayan, Athul and Mody, Tejasvinee Atul and Yu, Qin and Wolny, Adrian and Cerrone, Lorenzo and Strauss, Soeren and Tsiantis, Miltos and Smith, Richard S and Hamprecht, Fred A and Kreshuk, Anna and others},\n  journal={Development},\n  volume={151},\n  number={14},\n  year={2024},\n  publisher={The Company of Biologists}\n}\n</code></pre>"},{"location":"chapters/plantseg/","title":"Run PlantSeg: A Guide","text":"<ul> <li>Installation<ul> <li>Install Miniconda</li> <li>Install <code>plant-seg</code> using <code>mamba</code></li> </ul> </li> <li>Inference<ul> <li>Example configuration file for both training and inference</li> <li>Prediction</li> <li>Specifying a Graphic Card (GPU)</li> </ul> </li> <li>Cite</li> <li>PlantSeg Version and Code</li> </ul>"},{"location":"chapters/plantseg/#installation","title":"Installation","text":"<p>It is recommended to install this package with <code>mamba</code> (see below). If you don't have <code>mamba</code> installed, you can install it with <code>conda</code>. We start the guide by installing Mini-<code>conda</code>.</p>"},{"location":"chapters/plantseg/#install-miniconda","title":"Install Miniconda","text":"<p>First step required to use the pipeline is installing Miniconda. If you already have a working Anaconda setup you can go directly to the next step. Anaconda can be downloaded for all platforms from here. We suggest to use Miniconda, because it is lighter and install fewer unnecessary packages.</p> <p>To download Miniconda, open a terminal and type:</p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Then install by typing:</p> <pre><code>bash ./Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>and follow the installation instructions. The Miniconda3-latest-Linux-x86_64.sh file can be safely deleted.</p>"},{"location":"chapters/plantseg/#install-plant-seg-using-mamba","title":"Install <code>plant-seg</code> using <code>mamba</code>","text":"<p>Fist step is to install mamba, which is an alternative to conda:</p> <pre><code>conda install -c conda-forge mamba\n</code></pre> <p>PlantSeg version &gt;= v1.6.2 is required. If you have a nvidia gpu, install <code>plant-seg</code> using:</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=12.1 pyqt lcerrone::plantseg\n</code></pre> <p>or if you don't have a nvidia gpu, install <code>plant-seg</code> using:</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch cpuonly pyqt lcerrone::plantseg\n</code></pre>"},{"location":"chapters/plantseg/#inference","title":"Inference","text":""},{"location":"chapters/plantseg/#example-configuration-file-for-both-training-and-inference","title":"Example configuration file for both training and inference","text":"<p>The original configuration file used for training the final UNet PlantSeg model published on Bioimage.IO for wide applicability can be found at <code>plantseg/configs/config_train_final.yml</code>, which is a configuration file for pytorch-3dunet, the core network of PlantSeg.</p> <p>An example config file for segmentation can be found at <code>plantseg/configs/config_pred_wide_applicability.yaml</code>. To modify it and use it for your own data, you need to change the <code>path</code> parameters:</p> <ul> <li><code>path</code>: path to the folder containing the images to be segmented or to the image to be segmented</li> </ul> <p>You may also need to change these parameters:</p> <ul> <li><code>preprocessing:factor</code>: a rescale factor to match the nucleus size of your data to the training data, not necessary but may help in specific cases</li> <li><code>cnn_prediction:patch</code>: patch size should be smaller than the dimension of your image, and smaller than the GPU memory</li> </ul> <p>The full configuration file is shown below:</p> <pre><code># Contains the path to the directory or file to process\npath: PATH_TO_YOUR_DATA\n\npreprocessing:\n  # enable/disable preprocessing\n  state: True\n  # key for H5 or ZARR, can be set to null if only one key exists in each file\n  key: Null\n  # channel to use if input image has shape CZYX or CYX, otherwise set to null\n  channel: Null\n  # create a new sub folder where all results will be stored\n  save_directory: 'PreProcessing'\n  # rescaling the volume is essential for the generalization of the networks. The rescaling factor can be computed as the resolution\n  # of the volume at hand divided by the resolution of the dataset used in training. Be careful, if the difference is too large check for a different model.\n  factor: [1.0, 1.0, 1.0]\n  # the order of the spline interpolation\n  order: 2\n  # cropping out areas of little interest can drastically improve the performance of plantseg.\n  # crop volume has to be input using the numpy slicing convention [b_z:e_z, b_x:e_x, b_y:e_y], where b_zxy is the\n  # first point of a bounding box and e_zxy is the second. eg: [:, 100:500, 400:900]\n  crop_volume: '[:,:,:]'\n  # optional: perform Gaussian smoothing or median filtering on the input.\n  filter:\n    # enable/disable filtering\n    state: False\n    # Accepted values: 'gaussian'/'median'\n    type: gaussian\n    # sigma (gaussian) or disc radius (median)\n    filter_param: 1.0\n\ncnn_prediction:\n  # enable/disable UNet prediction\n  state: True\n  # key for H5 or ZARR, can be set to null if only one key exists in each file; null is recommended if the previous steps has state True\n  key: Null\n  # channel to use if input image has shape CZYX or CYX, otherwise set to null; null is recommended if the previous steps has state True\n  channel: Null\n  # Trained model name, more info on available models and custom models in PlantSeg documentation\n  model_name: 'PlantSeg_3Dnuc_platinum'\n  # If a CUDA capable gpu is available and corrected setup use \"cuda\", if not you can use \"cpu\" for cpu only inference (slower)\n  device: 'cuda'\n  # (int or tuple) padding to be removed from each axis in a given patch in order to avoid checkerboard artifacts\n  patch_halo: [64, 64, 64]\n  # how many subprocesses to use for data loading\n  num_workers: 8\n  # patch size given to the network (adapt to fit in your GPU mem)\n  patch: [192, 256, 256]\n  # stride between patches will be computed as `stride_ratio * patch`\n  # recommended values are in range `[0.5, 0.75]` to make sure the patches have enough overlap to get smooth prediction maps\n  stride_ratio: 0.50\n  # If \"True\" forces downloading networks from the online repos\n  model_update: False\n\ncnn_postprocessing:\n  # enable/disable cnn post processing\n  state: True\n  # key for H5 or ZARR, can be set to null if only one key exists in each file; null is recommended if the previous steps has state True\n  key: Null\n  # channel to use if input image has shape CZYX or CYX, otherwise set to null; null is recommended if the previous steps has state True\n  channel: 1\n  # if True convert to result to tiff\n  tiff: True\n  # rescaling factor\n  factor: [1, 1, 1]\n  # spline order for rescaling\n  order: 2\n\nsegmentation:\n  # enable/disable segmentation\n  state: True\n  # key for H5 or ZARR, can be set to null if only one key exists in each file; null is recommended if the previous steps has state True\n  key: 'predictions'\n  # channel to use if prediction has shape CZYX or CYX, otherwise set to null; null is recommended if the previous steps has state True\n  channel: 1\n  # Name of the algorithm to use for inferences. Options: MultiCut, MutexWS, GASP, DtWatershed\n  name: 'GASP'\n  # Segmentation specific parameters here\n  # balance under-/over-segmentation; 0 - aim for undersegmentation, 1 - aim for oversegmentation. (Not active for DtWatershed)\n  beta: 0.5\n  # directory where to save the results\n  save_directory: 'GASP'\n  # enable/disable watershed\n  run_ws: True\n  # use 2D instead of 3D watershed\n  ws_2D: False\n  # probability maps threshold\n  ws_threshold: 0.4\n  # set the minimum superpixels size\n  ws_minsize: 50\n  # sigma for the gaussian smoothing of the distance transform\n  ws_sigma: 2.0\n  # sigma for the gaussian smoothing of boundary\n  ws_w_sigma: 0\n  # set the minimum segment size in the final segmentation. (Not active for DtWatershed)\n  post_minsize: 100\n\nsegmentation_postprocessing:\n  # enable/disable segmentation post processing\n  state: True\n  # key for H5 or ZARR, can be set to null if only one key exists in each file; null is recommended if the previous steps has state True\n  key: Null\n  # channel to use if input image has shape CZYX or CYX, otherwise set to null; null is recommended if the previous steps has state True\n  channel: Null\n  # if True convert to result to tiff\n  tiff: True\n  # rescaling factor\n  factor: [1, 1, 1]\n  # spline order for rescaling (keep 0 for segmentation post processing\n  order: 0\n  # save raw input in the output segmentation file h5 file\n  save_raw: False\n</code></pre>"},{"location":"chapters/plantseg/#prediction","title":"Prediction","text":"<pre><code>plantseg --config CONFIG_PATH\n</code></pre> <p>where CONFIG_PATH is the path to the YAML configuration file. For example, if you want to use the model with the example configuration file <code>configs/config_pred_wide_applicability.yaml</code>:</p> <pre><code>cd ovules-instance-segmentation/plantseg/\nCUDA_VISIBLE_DEVICES=0 plantseg --config configs/train_and_infer.yml\n</code></pre>"},{"location":"chapters/plantseg/#specifying-a-graphic-card-gpu","title":"Specifying a Graphic Card (GPU)","text":"<p>If you need to specify a graphic card, for example to use the No. 7 card (the eighth), do:</p> <pre><code>CUDA_VISIBLE_DEVICES=7 plantseg --config CONFIG_PATH\n</code></pre> <p>If you have only one graphic card, use <code>CUDA_VISIBLE_DEVICES=0</code> to select the first card (No. 0).</p>"},{"location":"chapters/plantseg/#cite","title":"Cite","text":"<p>If you find this work useful, please cite both papers:</p> <pre><code>@article{vijayan2024deep,\n  title={A deep learning-based toolkit for 3D nuclei segmentation and quantitative analysis in cellular and tissue context},\n  author={Vijayan, Athul and Mody, Tejasvinee Atul and Yu, Qin and Wolny, Adrian and Cerrone, Lorenzo and Strauss, Soeren and Tsiantis, Miltos and Smith, Richard S and Hamprecht, Fred A and Kreshuk, Anna and others},\n  journal={Development},\n  volume={151},\n  number={14},\n  year={2024},\n  publisher={The Company of Biologists}\n}\n\n@article{wolny2020accurate,\n  title={Accurate and versatile 3D segmentation of plant tissues at cellular resolution},\n  author={Wolny, Adrian and Cerrone, Lorenzo and Vijayan, Athul and Tofanelli, Rachele and Barro, Amaya Vilches and Louveaux, Marion and Wenzl, Christian and Strauss, S{\\\"o}ren and Wilson-S{\\'a}nchez, David and Lymbouridou, Rena and others},\n  journal={Elife},\n  volume={9},\n  pages={e57613},\n  year={2020},\n  publisher={eLife Sciences Publications Limited}\n}\n</code></pre>"},{"location":"chapters/plantseg/#plantseg-version-and-code","title":"PlantSeg Version and Code","text":"<p>See PlantSeg's website for more details. The PlantSeg version v1.4.3 was used for testing, and PlantSeg v1.6.2 was released for this paper.</p>"},{"location":"chapters/stardist/","title":"Run Stardist: A Guide and Pipeline","text":"<p>A complete training and inference pipeline for 3D StarDist with an example on 3D biological (ovules) datasets. If you encounter any issues or have suggestions, please submit an issue.</p> <ul> <li>Models and Data<ul> <li>Use Pre-trained Model</li> <li>Training Data Statistics and Links</li> </ul> </li> <li>Installation<ul> <li>Install Miniconda</li> <li>Install <code>run-stardist</code> using <code>mamba</code></li> </ul> </li> <li>Usage<ul> <li>Example Configuration File for Training and Inference</li> <li>Training</li> <li>Prediction</li> <li>Specifying a Graphics Card (GPU)</li> <li>Memory Profiling</li> </ul> </li> <li>Cite</li> </ul>"},{"location":"chapters/stardist/#models-and-data","title":"Models and Data","text":"<p>A 3D nucleus segmentation model is available for download from Bioimage.IO and is ready to use for segmenting nuclei. The model was trained on a 3D confocal ovule dataset from Arabidopsis thaliana using StarDist version v0.8.3.</p>"},{"location":"chapters/stardist/#use-pre-trained-model","title":"Use Pre-trained Model","text":"<p>Model weights and related files can be found at DOI 10.5281/zenodo.8421755. The programme automatically downloads the model for inference if <code>generic_plant_nuclei_3D</code> is specified as <code>model_name</code> in the configuration file.</p> <p>Currently, this is the only 3D StarDist model available on Bioimage Model Zoo. If you have another model, place its folder in <code>PATH_TO_MODEL_DIR</code> and specify the folder name as <code>MY_MODEL_NAME</code> in the configuration file. Then run <code>predict-stardist</code> to use the model for inference. See the Prediction section for more details.</p>"},{"location":"chapters/stardist/#training-data-statistics-and-links","title":"Training Data Statistics and Links","text":"<p>The training data is publicly available on Zenodo at BioImage Archive S-BIAD1026. Key statistics include:</p> <pre><code>original_voxel_size = {  # z, y, x\n    1135: [0.2837, 0.1268, 0.1268],  # validation\n    1136: [0.2837, 0.1276, 0.1276],  # training\n    1137: [0.2837, 0.1266, 0.1266],  # training\n    1139: [0.2799, 0.1267, 0.1267],  # training\n    1170: [0.2780, 0.1270, 0.1270],  # training\n}  # Median: [0.2837, 0.1268, 0.1268]\n</code></pre> <pre><code>original_median_extents = {  # z, y, x\n    1135: [16, 32, 33],  # validation\n    1136: [16, 32, 32],  # training\n    1137: [16, 32, 32],  # training\n    1139: [16, 32, 33],  # training\n    1170: [16, 29, 30],  # training\n}  # Median: [16, 32, 32]\n</code></pre>"},{"location":"chapters/stardist/#installation","title":"Installation","text":"<p>It is recommended to install this package with <code>mamba</code>. If you don't have <code>mamba</code>, install it via <code>conda</code> or refer to the official installation guide. We start by installing Miniconda.</p> <ul> <li>Memory requirement: Memory profiling shows that running the prediction on one 3D volume 1135 requires 24.0 GB of RAM, and running on all five volumes of training data requires 26.9 GB of RAM. I recommend to use a machine with 32 GB of RAM or more.</li> <li>For Windows users: TensorFlow has no Conda build for Windows. If using Windows, install TensorFlow from its official website and use <code>mamba</code> to install the other dependencies (without <code>tensorflow</code>).</li> </ul>"},{"location":"chapters/stardist/#install-miniconda","title":"Install Miniconda","text":"<p>Download Miniconda:</p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Install Miniconda:</p> <pre><code>bash ./Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Follow the on-screen instructions. The installer file can be deleted afterward.</p>"},{"location":"chapters/stardist/#install-run-stardist-using-mamba","title":"Install <code>run-stardist</code> using <code>mamba</code>","text":"<p>First, install <code>mamba</code>:</p> <pre><code>conda install -c conda-forge mamba\n</code></pre> <p>For systems with an NVIDIA GPU, install <code>run-stardist</code> using:</p> <pre><code>mamba create -n run-stardist -c qin-yu -c conda-forge \"python&gt;=3.10\" tensorflow stardist wandb pydantic run-stardist\n</code></pre> <p>For systems without an NVIDIA GPU, install <code>run-stardist</code> using:</p> <pre><code>mamba create -n run-stardist -c qin-yu -c conda-forge \"python&gt;=3.10\" tensorflow-cpu stardist wandb pydantic run-stardist\n</code></pre>"},{"location":"chapters/stardist/#usage","title":"Usage","text":""},{"location":"chapters/stardist/#example-configuration-file-for-training-and-inference","title":"Example Configuration File for Training and Inference","text":"<p>The original configuration file used for training the final ResNet StarDist model published on Bioimage.IO for wide applicability can be found at <code>stardist/configs/final_resnet_model_config.yml</code>, which can used for both training and inference (note that the inference output is only used for illustration in this repository because it's segmenting the training data).</p> <p>The generic template is shown below. A configuration template with more guidelines can be found at <code>stardist/configs/train_and_infer.yml</code>.</p> <pre><code>wandb: # optional, remove this part if not using W&amp;B\n  project: ovules-instance-segmentation\n  name: final-stardist-model\n\ndata:\n  # Rescale outside StarDist\n  rescale_factor: Null\n\n  # Training (ignored in inference config file)\n  training:\n    - PATH_TO_INPUT_DIR_1 or PATH_TO_INPUT_FILE_1\n    - PATH_TO_INPUT_DIR_2 or PATH_TO_INPUT_FILE_2\n  validation:\n    - PATH_TO_INPUT_DIR_3 or PATH_TO_INPUT_FILE_3\n  raw_name: raw/noisy      # only required if HDF5\n  label_name: label/gold   # only required if HDF5\n\n  # Inference (ignored in training config file)\n  prediction:\n    - PATH_TO_INPUT_DIR_4 or PATH_TO_INPUT_FILE_4\n    - PATH_TO_INPUT_DIR_5 or PATH_TO_INPUT_FILE_5\n  format: tiff             # only 'hdf5' or 'tiff'\n  name: raw/nuclei         # dataset name of the raw image in HDF5 files, only required if format is `hdf5`\n  output_dir: MY_OUTPUT_DIR\n  output_dtype: uint16     # `uint8`, `uint16`, or `float32` are recommended\n  resize_to_original: True # output should be of he same shape as input\n  target_voxel_size: Null  # the desired voxel size to rescale to during inference, null if rescale factor is set\n  save_probability_map: True\n\nstardist:\n  model_dir: PATH_TO_MODEL_DIR  # set to `null` if model name is `generic_plant_nuclei_3D`\n  model_name: MY_MODEL_NAME     # set to `generic_plant_nuclei_3D` to use the builtin model\n  model_type: StarDist3D\n  model_config:  # model configuration should stay identical for training and inference\n    backbone: resnet\n    n_rays: 96\n    grid: [2, 4, 4]\n    use_gpu: False\n    n_channel_in: 1\n    patch_size: [96, 96, 96]  # multiple of 16 prefered\n    train_batch_size: 8\n    train_n_val_patches: 16\n    steps_per_epoch: 400\n    epochs: 1000\n\naugmenter:\n  name: default\n</code></pre>"},{"location":"chapters/stardist/#training","title":"Training","text":"<pre><code>train-stardist --config CONFIG_PATH\n</code></pre> <p>where CONFIG_PATH is the path to the YAML configuration file. For example, if you want to train the model with the example configuration file <code>configs/train_and_infer.yml</code>:</p> <pre><code>cd go-nuclear/stardist/\nCUDA_VISIBLE_DEVICES=0 train-stardist --config configs/train_and_infer.yml\n</code></pre>"},{"location":"chapters/stardist/#prediction","title":"Prediction","text":"<pre><code>predict-stardist --config CONFIG_PATH\n</code></pre> <p>where CONFIG_PATH is the path to the YAML configuration file. For example, if you want to use the model with the example configuration file <code>configs/train_and_infer.yml</code>:</p> <pre><code>cd go-nuclear/stardist/\nCUDA_VISIBLE_DEVICES=0 predict-stardist --config configs/train_and_infer.yml\n</code></pre> <p>Preprocessing: For the published StarDist Plant Nuclei 3D ResNet the median size of nuclei in training data is <code>[16, 32, 32]</code>. To achieve the best segmentation result, the input 3D images should be rescaled so that your nucleus size in ZYX matches the training data. For example, if the median nucleus size of your data is <code>[32, 32, 32]</code>, then <code>rescale_factor</code> should be <code>[0.5, 1., 1.]</code>; if it's <code>[15, 33, 31]</code>, then it does not have to be rescaled. You may also choose to leave <code>rescale_factor</code> as <code>Null</code> and rescale your images with Fiji or other tools before running the pipeline. If <code>resize_to_original</code> is <code>True</code> then the output will have the original size of the input image.</p>"},{"location":"chapters/stardist/#specifying-a-graphics-card-gpu","title":"Specifying a Graphics Card (GPU)","text":"<p>If you need to specify a graphic card, for example to use the No. 7 card (the eighth), do:</p> <pre><code>CUDA_VISIBLE_DEVICES=7 predict-stardist --config CONFIG_PATH\n</code></pre> <p>If you have only one graphic card, use <code>CUDA_VISIBLE_DEVICES=0</code> to select the first card (No. 0).</p>"},{"location":"chapters/stardist/#memory-profiling","title":"Memory Profiling","text":"<p>To run the code on 5 training volumes, the peak memory usage is 26.9 GiB:</p> <pre><code>Command line: memray run predict.py --config /yu/go-nuclear/stardist/configs/final_resnet_model_config.yml\nDuration: 0:46:31.577000\nTotal number of allocations: 1735922785\nTotal number of frames seen: 24615\nPeak memory usage: 26.9 GiB\nPython allocator: pymalloc\n</code></pre>"},{"location":"chapters/stardist/#cite","title":"Cite","text":"<p>If you use this code, model, or dataset, please cite:</p> <pre><code>@article{vijayan2024deep,\n  title={A deep learning-based toolkit for 3D nuclei segmentation and quantitative analysis in cellular and tissue context},\n  author={Vijayan, Athul and Mody, Tejasvinee Atul and Yu, Qin and Wolny, Adrian and Cerrone, Lorenzo and Strauss, Soeren and Tsiantis, Miltos and Smith, Richard S and Hamprecht, Fred A and Kreshuk, Anna and others},\n  journal={Development},\n  volume={151},\n  number={14},\n  year={2024},\n  publisher={The Company of Biologists}\n}\n\n@inproceedings{weigert2020star,\n  title={Star-convex polyhedra for 3D object detection and segmentation in microscopy},\n  author={Weigert, Martin and Schmidt, Uwe and Haase, Robert and Sugawara, Ko and Myers, Gene},\n  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},\n  pages={3666--3673},\n  year={2020}\n}\n</code></pre>"}]}